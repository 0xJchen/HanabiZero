[2022-01-17 09:50:37,048][train][INFO][train.py>_log] ==> #0          Total Loss: 54.828   [weighted Loss:54.828   Policy Loss: 13.727   Value Loss: 37.125   Reward Loss: 31.820   Consistency Loss: 0.000    ] Replay Episodes Collected: 2330       Buffer Size: 2330       Transition Number: 24.938  k Batch Size: 256        Lr: 0.00000 
[2022-01-17 09:52:43,087][train][INFO][train.py>_log] ==> #1000       Total Loss: 5.502    [weighted Loss:5.502    Policy Loss: 15.333   Value Loss: 4.080    Reward Loss: 1.079    Consistency Loss: 0.000    ] Replay Episodes Collected: 14226      Buffer Size: 14226      Transition Number: 176.746 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 09:54:47,690][train][INFO][train.py>_log] ==> #2000       Total Loss: 6.141    [weighted Loss:6.141    Policy Loss: 14.741   Value Loss: 3.689    Reward Loss: 0.976    Consistency Loss: 0.000    ] Replay Episodes Collected: 25767      Buffer Size: 25767      Transition Number: 318.649 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 09:56:53,230][train][INFO][train.py>_log] ==> #3000       Total Loss: 5.411    [weighted Loss:5.411    Policy Loss: 13.731   Value Loss: 3.438    Reward Loss: 1.208    Consistency Loss: 0.000    ] Replay Episodes Collected: 39027      Buffer Size: 39027      Transition Number: 462.208 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 09:59:04,155][train][INFO][train.py>_log] ==> #4000       Total Loss: 4.048    [weighted Loss:4.048    Policy Loss: 13.031   Value Loss: 3.347    Reward Loss: 1.264    Consistency Loss: 0.000    ] Replay Episodes Collected: 53583      Buffer Size: 53583      Transition Number: 620.584 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:01:36,080][train][INFO][train.py>_log] ==> #5000       Total Loss: 4.365    [weighted Loss:4.365    Policy Loss: 13.291   Value Loss: 3.129    Reward Loss: 1.086    Consistency Loss: 0.000    ] Replay Episodes Collected: 72416      Buffer Size: 72416      Transition Number: 810.829 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:04:32,501][train][INFO][train.py>_log] ==> #6000       Total Loss: 4.492    [weighted Loss:4.492    Policy Loss: 12.715   Value Loss: 3.163    Reward Loss: 1.180    Consistency Loss: 0.000    ] Replay Episodes Collected: 94168      Buffer Size: 94168      Transition Number: 1027.918k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:07:42,516][train][INFO][train.py>_log] ==> #7000       Total Loss: 4.255    [weighted Loss:4.255    Policy Loss: 13.011   Value Loss: 3.243    Reward Loss: 1.046    Consistency Loss: 0.000    ] Replay Episodes Collected: 114584     Buffer Size: 114584     Transition Number: 1271.890k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:11:44,786][train][INFO][train.py>_log] ==> #8000       Total Loss: 4.088    [weighted Loss:4.088    Policy Loss: 14.278   Value Loss: 2.902    Reward Loss: 1.103    Consistency Loss: 0.000    ] Replay Episodes Collected: 140369     Buffer Size: 133203     Transition Number: 1499.997k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:16:15,688][train][INFO][train.py>_log] ==> #9000       Total Loss: 3.087    [weighted Loss:3.087    Policy Loss: 13.722   Value Loss: 2.711    Reward Loss: 0.968    Consistency Loss: 0.000    ] Replay Episodes Collected: 167020     Buffer Size: 129223     Transition Number: 1499.988k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:20:39,885][train][INFO][train.py>_log] ==> #10000      Total Loss: 2.496    [weighted Loss:2.496    Policy Loss: 13.160   Value Loss: 2.837    Reward Loss: 0.846    Consistency Loss: 0.000    ] Replay Episodes Collected: 192928     Buffer Size: 121117     Transition Number: 1499.998k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:25:05,585][train][INFO][train.py>_log] ==> #11000      Total Loss: 3.438    [weighted Loss:3.438    Policy Loss: 12.588   Value Loss: 2.601    Reward Loss: 0.904    Consistency Loss: 0.000    ] Replay Episodes Collected: 218874     Buffer Size: 113624     Transition Number: 1499.998k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:29:38,374][train][INFO][train.py>_log] ==> #12000      Total Loss: 3.189    [weighted Loss:3.189    Policy Loss: 10.336   Value Loss: 2.725    Reward Loss: 0.919    Consistency Loss: 0.000    ] Replay Episodes Collected: 244984     Buffer Size: 110086     Transition Number: 1500.189k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:34:54,215][train][INFO][train.py>_log] ==> #13000      Total Loss: 2.922    [weighted Loss:2.922    Policy Loss: 11.731   Value Loss: 3.056    Reward Loss: 1.098    Consistency Loss: 0.000    ] Replay Episodes Collected: 283140     Buffer Size: 116278     Transition Number: 1500.084k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:40:14,105][train][INFO][train.py>_log] ==> #14000      Total Loss: 3.192    [weighted Loss:3.192    Policy Loss: 11.596   Value Loss: 2.917    Reward Loss: 0.923    Consistency Loss: 0.000    ] Replay Episodes Collected: 321378     Buffer Size: 123999     Transition Number: 1499.978k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:45:18,440][train][INFO][train.py>_log] ==> #15000      Total Loss: 3.697    [weighted Loss:3.697    Policy Loss: 10.292   Value Loss: 3.210    Reward Loss: 1.130    Consistency Loss: 0.000    ] Replay Episodes Collected: 358549     Buffer Size: 131144     Transition Number: 1500.000k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:50:13,056][train][INFO][train.py>_log] ==> #16000      Total Loss: 2.123    [weighted Loss:2.123    Policy Loss: 7.929    Value Loss: 2.724    Reward Loss: 0.938    Consistency Loss: 0.000    ] Replay Episodes Collected: 393299     Buffer Size: 135677     Transition Number: 1499.991k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:53:44,065][train][INFO][train.py>_log] ==> #17000      Total Loss: 2.137    [weighted Loss:2.137    Policy Loss: 9.525    Value Loss: 2.939    Reward Loss: 0.888    Consistency Loss: 0.000    ] Replay Episodes Collected: 404395     Buffer Size: 125617     Transition Number: 1499.992k Batch Size: 256        Lr: 0.10000 
[2022-01-17 10:57:19,937][train][INFO][train.py>_log] ==> #18000      Total Loss: 3.711    [weighted Loss:3.711    Policy Loss: 11.512   Value Loss: 3.184    Reward Loss: 0.708    Consistency Loss: 0.000    ] Replay Episodes Collected: 415925     Buffer Size: 115710     Transition Number: 1499.995k Batch Size: 256        Lr: 0.10000 
[2022-01-17 11:01:18,149][train][INFO][train.py>_log] ==> #19000      Total Loss: 2.281    [weighted Loss:2.281    Policy Loss: 11.792   Value Loss: 3.400    Reward Loss: 0.829    Consistency Loss: 0.000    ] Replay Episodes Collected: 431676     Buffer Size: 104175     Transition Number: 1499.991k Batch Size: 256        Lr: 0.10000 
[2022-01-17 11:05:22,935][train][INFO][train.py>_log] ==> #20000      Total Loss: 3.587    [weighted Loss:3.587    Policy Loss: 10.484   Value Loss: 3.552    Reward Loss: 0.951    Consistency Loss: 0.000    ] Replay Episodes Collected: 447954     Buffer Size: 93725      Transition Number: 1499.926k Batch Size: 256        Lr: 0.10000 
