[2022-01-17 15:09:03,885][train][INFO][train.py>_log] ==> #0          Total Loss: 47.988   [weighted Loss:47.988   Policy Loss: 13.516   Value Loss: 31.821   Reward Loss: 26.517   Consistency Loss: 0.000    ] Replay Episodes Collected: 1931       Buffer Size: 1931       Transition Number: 21.585  k Batch Size: 256        Lr: 0.00000 
[2022-01-17 15:10:54,472][train][INFO][train.py>_log] ==> #1000       Total Loss: 4.762    [weighted Loss:4.762    Policy Loss: 12.717   Value Loss: 3.642    Reward Loss: 0.982    Consistency Loss: 0.000    ] Replay Episodes Collected: 9314       Buffer Size: 9314       Transition Number: 116.184 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 15:12:41,320][train][INFO][train.py>_log] ==> #2000       Total Loss: 5.478    [weighted Loss:5.478    Policy Loss: 12.922   Value Loss: 3.267    Reward Loss: 0.930    Consistency Loss: 0.000    ] Replay Episodes Collected: 16722      Buffer Size: 16722      Transition Number: 208.327 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 15:14:23,850][train][INFO][train.py>_log] ==> #3000       Total Loss: 4.342    [weighted Loss:4.342    Policy Loss: 11.788   Value Loss: 3.144    Reward Loss: 1.052    Consistency Loss: 0.000    ] Replay Episodes Collected: 25064      Buffer Size: 25064      Transition Number: 293.029 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 15:16:10,191][train][INFO][train.py>_log] ==> #4000       Total Loss: 4.876    [weighted Loss:4.876    Policy Loss: 11.340   Value Loss: 2.842    Reward Loss: 0.987    Consistency Loss: 0.000    ] Replay Episodes Collected: 34003      Buffer Size: 34003      Transition Number: 382.671 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 15:17:57,204][train][INFO][train.py>_log] ==> #5000       Total Loss: 4.204    [weighted Loss:4.204    Policy Loss: 10.609   Value Loss: 2.773    Reward Loss: 0.999    Consistency Loss: 0.000    ] Replay Episodes Collected: 42450      Buffer Size: 42450      Transition Number: 470.700 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 15:19:47,475][train][INFO][train.py>_log] ==> #6000       Total Loss: 4.194    [weighted Loss:4.194    Policy Loss: 12.242   Value Loss: 3.045    Reward Loss: 1.173    Consistency Loss: 0.000    ] Replay Episodes Collected: 51405      Buffer Size: 51405      Transition Number: 564.071 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 15:21:40,594][train][INFO][train.py>_log] ==> #7000       Total Loss: 5.325    [weighted Loss:5.325    Policy Loss: 12.231   Value Loss: 2.482    Reward Loss: 0.797    Consistency Loss: 0.000    ] Replay Episodes Collected: 57011      Buffer Size: 57011      Transition Number: 655.959 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 15:23:39,175][train][INFO][train.py>_log] ==> #8000       Total Loss: 4.911    [weighted Loss:4.911    Policy Loss: 11.325   Value Loss: 2.412    Reward Loss: 0.907    Consistency Loss: 0.000    ] Replay Episodes Collected: 62625      Buffer Size: 62625      Transition Number: 755.146 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 15:26:02,288][train][INFO][train.py>_log] ==> #9000       Total Loss: 3.162    [weighted Loss:3.162    Policy Loss: 10.581   Value Loss: 2.359    Reward Loss: 1.017    Consistency Loss: 0.000    ] Replay Episodes Collected: 75961      Buffer Size: 75961      Transition Number: 888.689 k Batch Size: 256        Lr: 0.10000 
[2022-01-17 15:28:48,614][train][INFO][train.py>_log] ==> #10000      Total Loss: 3.597    [weighted Loss:3.597    Policy Loss: 11.476   Value Loss: 2.410    Reward Loss: 1.024    Consistency Loss: 0.000    ] Replay Episodes Collected: 92273      Buffer Size: 88581      Transition Number: 1000.002k Batch Size: 256        Lr: 0.10000 
