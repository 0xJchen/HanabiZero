[2022-03-04 01:38:34,766][train][INFO][train.py>_log] ==> #0          Total Loss: 48.093   [weighted Loss:48.093   Policy Loss: 13.622   Value Loss: 31.821   Reward Loss: 26.517   Consistency Loss: 0.000    ] Replay Episodes Collected: 1770       Buffer Size: 1770       Transition Number: 18.716  k Batch Size: 256        Lr: 0.00000 
[2022-03-04 01:42:18,415][train][INFO][train.py>_log] ==> #1000       Total Loss: 7.354    [weighted Loss:7.354    Policy Loss: 14.024   Value Loss: 5.209    Reward Loss: 2.073    Consistency Loss: 0.000    ] Replay Episodes Collected: 18899      Buffer Size: 18899      Transition Number: 232.619 k Batch Size: 256        Lr: 0.10000 
[2022-03-04 01:46:07,992][train][INFO][train.py>_log] ==> #2000       Total Loss: 6.461    [weighted Loss:6.461    Policy Loss: 14.008   Value Loss: 4.622    Reward Loss: 1.832    Consistency Loss: 0.000    ] Replay Episodes Collected: 36277      Buffer Size: 36277      Transition Number: 451.516 k Batch Size: 256        Lr: 0.20000 
[2022-03-04 01:49:57,329][train][INFO][train.py>_log] ==> #3000       Total Loss: 6.001    [weighted Loss:6.001    Policy Loss: 13.238   Value Loss: 4.615    Reward Loss: 1.952    Consistency Loss: 0.000    ] Replay Episodes Collected: 59099      Buffer Size: 59099      Transition Number: 673.318 k Batch Size: 256        Lr: 0.20000 
[2022-03-04 01:53:47,734][train][INFO][train.py>_log] ==> #4000       Total Loss: 6.995    [weighted Loss:6.995    Policy Loss: 12.762   Value Loss: 4.334    Reward Loss: 2.062    Consistency Loss: 0.000    ] Replay Episodes Collected: 82229      Buffer Size: 82229      Transition Number: 894.895 k Batch Size: 256        Lr: 0.20000 
[2022-03-04 01:57:38,831][train][INFO][train.py>_log] ==> #5000       Total Loss: 6.766    [weighted Loss:6.766    Policy Loss: 12.405   Value Loss: 3.968    Reward Loss: 1.971    Consistency Loss: 0.000    ] Replay Episodes Collected: 108958     Buffer Size: 99257      Transition Number: 1000.450k Batch Size: 256        Lr: 0.20000 
[2022-03-04 02:01:36,017][train][INFO][train.py>_log] ==> #6000       Total Loss: 6.386    [weighted Loss:6.386    Policy Loss: 12.396   Value Loss: 4.001    Reward Loss: 2.165    Consistency Loss: 0.000    ] Replay Episodes Collected: 136374     Buffer Size: 108412     Transition Number: 1000.168k Batch Size: 256        Lr: 0.20000 
[2022-03-04 02:05:42,477][train][INFO][train.py>_log] ==> #7000       Total Loss: 4.273    [weighted Loss:4.273    Policy Loss: 11.800   Value Loss: 3.814    Reward Loss: 2.124    Consistency Loss: 0.000    ] Replay Episodes Collected: 165126     Buffer Size: 115082     Transition Number: 1000.166k Batch Size: 256        Lr: 0.20000 
[2022-03-04 02:09:40,541][train][INFO][train.py>_log] ==> #8000       Total Loss: 5.311    [weighted Loss:5.311    Policy Loss: 11.173   Value Loss: 3.473    Reward Loss: 1.977    Consistency Loss: 0.000    ] Replay Episodes Collected: 192705     Buffer Size: 118463     Transition Number: 1000.235k Batch Size: 256        Lr: 0.20000 
[2022-03-04 02:13:47,072][train][INFO][train.py>_log] ==> #9000       Total Loss: 5.570    [weighted Loss:5.570    Policy Loss: 11.006   Value Loss: 3.557    Reward Loss: 2.148    Consistency Loss: 0.000    ] Replay Episodes Collected: 228801     Buffer Size: 126677     Transition Number: 1000.563k Batch Size: 256        Lr: 0.20000 
[2022-03-04 02:17:49,841][train][INFO][train.py>_log] ==> #10000      Total Loss: 2.317    [weighted Loss:2.317    Policy Loss: 6.325    Value Loss: 3.650    Reward Loss: 2.377    Consistency Loss: 0.000    ] Replay Episodes Collected: 264557     Buffer Size: 133598     Transition Number: 1000.264k Batch Size: 256        Lr: 0.20000 
